{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"yarn\").config('spark.executor.instances','100').appName(\"chiang_ct\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df = spark.read.csv(\"/user/spark/share/splitdata.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('behavior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_cate = spark.sql(\"select distinct user_cate from behavior\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cate.select('user_cate').distinct().count() #just checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.sql(\"select * from behavior where time_stamp < 1493424000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('behavior_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_pv1 = spark.sql('select user_cate as user_cate_pv1, count(*) as pv_1 from behavior_1 where btag = \"pv\" group by user_cate')\n",
    "%time df_cart1 = spark.sql('select user_cate as user_cate_cart1, count(*) as cart_1 from behavior_1 where btag = \"cart\" group by user_cate')\n",
    "%time df_fav1 = spark.sql('select user_cate as user_cate_fav1, count(*) as fav_1 from behavior_1 where btag = \"fav\" group by user_cate')\n",
    "%time df_buy1 = spark.sql('select user_cate as user_cate_buy1, count(*) as buy_1 from behavior_1 where btag = \"buy\" group by user_cate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_pv1.show() #just checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time print(df_pv1.count()) #just checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.select('user_cate').distinct().count()) #just checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pv1.select('user_cate_pv1').distinct().count()) #just checking\n",
    "print(df_cart1.select('user_cate_cart1').distinct().count()) #just checking\n",
    "print(df_fav1.select('user_cate_fav1').distinct().count()) #just checking\n",
    "print(df_buy1.select('user_cate_buy1').distinct().count()) #just checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "23029460 + 2828237 +1591969 +1843781 - 23296422"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join to cross_table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join to crosstable for table 2 with time interval 1 (2017/04/21 00:00:00 to 2017/04/29 00:00:00)\n",
    "tb2_cross_1 = df_user_cate.join(df_pv1,(df_user_cate.user_cate == df_pv1.user_cate_pv1) ,'outer')\n",
    "tb2_cross_1 = tb2_cross_1.join(df_cart1,(tb2_cross_1.user_cate == df_cart1.user_cate_cart1) ,'outer')\n",
    "tb2_cross_1 = tb2_cross_1.join(df_fav1,(tb2_cross_1.user_cate == df_fav1.user_cate_fav1) ,'outer')\n",
    "tb2_cross_1 = tb2_cross_1.join(df_buy1,(tb2_cross_1.user_cate == df_buy1.user_cate_buy1) ,'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb2_cross_1.columns #just for check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb2_cross_1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb2_cross_1.createOrReplaceTempView('tb2_cross_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spark.sql(\"select sum(pv_1) as sum_pv1, sum(cart_1) as sum_cart1, sum(fav_1) as sum_fav1, sum(buy_1) as sum_buy1 from tb2_cross_1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "202899898 + 4737124+2776525+2631753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tb2_cross_1.write.csv(\"hdfs:///user/spark/share/output/table2/cross/cross_1\",header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join to cross_table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.sql(\"select * from behavior where time_stamp between 1493424000 and 1494028800\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView('behavior_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_pv2 = spark.sql('select user_cate as user_cate_pv2, count(*) as pv_2 from behavior_2 where btag = \"pv\" group by user_cate')\n",
    "%time df_cart2 = spark.sql('select user_cate as user_cate_cart2, count(*) as cart_2 from behavior_2 where btag = \"cart\" group by user_cate')\n",
    "%time df_fav2 = spark.sql('select user_cate as user_cate_fav2, count(*) as fav_2 from behavior_2 where btag = \"fav\" group by user_cate')\n",
    "%time df_buy2 = spark.sql('select user_cate as user_cate_buy2, count(*) as buy_2 from behavior_2 where btag = \"buy\" group by user_cate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb2_cross_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join to crosstable for table 2 with time interval 2\n",
    "tb2_cross_all = tb2_cross_1.join(df_pv2,(tb2_cross_1.user_cate == df_pv2.user_cate_pv2) ,'outer')\n",
    "tb2_cross_all = tb2_cross_all.join(df_cart2,(tb2_cross_all.user_cate == df_cart2.user_cate_cart2) ,'outer')\n",
    "tb2_cross_all = tb2_cross_all.join(df_fav2,(tb2_cross_all.user_cate == df_fav2.user_cate_fav2) ,'outer')\n",
    "tb2_cross_all = tb2_cross_all.join(df_buy2,(tb2_cross_all.user_cate == df_buy2.user_cate_buy2) ,'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb2_cross_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join to cross_table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.sql(\"select * from behavior where time_stamp > 1494028800\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.createOrReplaceTempView('behavior_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_pv3 = spark.sql('select user_cate as user_cate_pv3, count(*) as pv_3 from behavior_3 where btag = \"pv\" group by user_cate')\n",
    "%time df_cart3 = spark.sql('select user_cate as user_cate_cart3, count(*) as cart_3 from behavior_3 where btag = \"cart\" group by user_cate')\n",
    "%time df_fav3 = spark.sql('select user_cate as user_cate_fav3, count(*) as fav_3 from behavior_3 where btag = \"fav\" group by user_cate')\n",
    "%time df_buy3 = spark.sql('select user_cate as user_cate_buy3, count(*) as buy_3 from behavior_3 where btag = \"buy\" group by user_cate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join to crosstable for table 2 with time interval 3\n",
    "tb2_cross_all = tb2_cross_all.join(df_pv3,(tb2_cross_all.user_cate == df_pv3.user_cate_pv3) ,'outer')\n",
    "tb2_cross_all = tb2_cross_all.join(df_cart3,(tb2_cross_all.user_cate == df_cart3.user_cate_cart3) ,'outer')\n",
    "tb2_cross_all = tb2_cross_all.join(df_fav3,(tb2_cross_all.user_cate == df_fav3.user_cate_fav3) ,'outer')\n",
    "tb2_cross_all = tb2_cross_all.join(df_buy3,(tb2_cross_all.user_cate == df_buy3.user_cate_buy3) ,'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb2_cross_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tb2_cross_all.write.csv(\"hdfs:///user/spark/share/output/table2/cross/tb2_cross_all\",header=\"true\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
